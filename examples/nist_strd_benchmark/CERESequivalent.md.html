                <meta charset="utf-8" emacsmode="-*- markdown -*-">
                            **CERES vs Opt**


One Iteration, CERES vs. Opt
===============================================================================


CERES directly modifies the Jacobian by multiplying it with a scaling matrix S.
S is a diagonal matrix where each entry 

$$s_{ii} = \frac{1}{1+\sqrt{(J^TJ)_{ii}}}$$

We can write this without resorting to entry-wise notation if we let

$$Q = \mbox{diag}(J^TJ)$$

and $U$ be the matrix such that

$$Q = U^TU$$

Then $S$ is just

$$S = (U + I)^{-1}$$

The following uses notation from:
https://en.wikipedia.org/wiki/Conjugate_gradient_method (The preconditioned conjugate gradient method)

Opt uses this notation internally (and uses a PCG solver), so this will be helpful to align our thoughts.

Using the preconditioner $M$, the preconditioned conjugate gradient solver is equivalent to solving:

$$E^{-1}A(E^{-1})^T\hat{x} = E^{-1}b$$

where $EE^T=M$ and $\hat{x} = E^Tx$.

I propose that CERES is solving exactly this system (and transforming $\hat{x}$ appropriately after solving), for:

$E^{-1} = S$, $A = J^TJ + \lambda I$, $\lambda = 1/\mbox{radius}$, $b = J^TF$

CERES claims to be solving the system 
$$(J^TJ + D^TD)\hat{x} = J^TF$$

but their J has already been multiplied by $S$, so the system really is 

$$((JS)^T(JS) + D^TD)\hat{x} = (JS)^Tr$$

Let $C$ be the matrix such that $D = CS$ ($S$ and $D$ are positive diagonal matrices, so such a $C$ is guaranteed to exist).

$$((JS)^T(JS) + (CS)^T(CS))\hat{x} = (JS)^Tr$$

since $(AB)^T = A^TB^T$, we have

$$(S^TJ^TJS + S^TC^TCS)\hat{x} = S^TJ^TF$$

by distributivity:

$$S^T(J^TJ + C^TC)S\hat{x} = S^TJ^TF$$

$S$ is again, diagonal, so $S^T = S$, thus:

$$S(J^TJ + C^TC)S^T\hat{x} = SJ^TF$$

So CERES is solving the system $E^{-1}A(E^{-1})^T\hat{x} = E^{-1}b$

with $E^{-1} = S$, $A = J^TJ + C^TC$, and $b = J^TF$.

This is close to what I proposed; they would be equivalent if $C^TC = I/\mbox{radius}$.

Let's see if this is indeed the case. Assume $C^TC = I/\mbox{radius}$.

We know each (diagonal) entry of $C^TC$ is $1/\mbox{radius}$, thus each diagonal entry of $C$ is $1/\sqrt{\mbox{radius}}$.

$CS = D$ and $s_{ii} = \frac{1}{1+\sqrt{(J^TJ)_{ii}}}$, then:


$$d_{ii} = c_{ii}s_{ii} = \frac{1}{\sqrt{\mbox{radius}}} \frac{1}{1+\sqrt{(J^TJ)_{ii}}}$$

$$d_{ii} = \frac{1}{\sqrt{\mbox{radius}}(1+\sqrt{(J^TJ)_{ii}})}$$

Let's take a look at CERES internals to see how it actually calculates $d_{ii}$.

It calculates:

$$K = diag((JS)^T(JS))$$

which is equivalently

$$K = diag(S^T(J^TJ)S)$$

Then

$$k_{ii} = s_{ii}^2(J^TJ)_{ii}$$

$$d_{ii} = \sqrt{k_{ii}/\mbox{radius}} = \sqrt{s_{ii}^2(J^TJ)_{ii}/\mbox{radius}}$$

$$d_{ii} = \sqrt{\frac{1}{1+\sqrt{(J^TJ)_{ii}}}^2(J^TJ)_{ii}/\mbox{radius}}$$

$$d_{ii} = \frac{1}{\sqrt{\mbox{radius}}}\sqrt{\frac{(J^TJ)_{ii}}{(1+\sqrt{(J^TJ)_{ii}})^2}}$$

$$d_{ii} = \frac{1}{\sqrt{\mbox{radius}}}\frac{\sqrt{(J^TJ)_{ii}}}{(1+\sqrt{(J^TJ)_{ii}})}$$

This is off from being the proposed value of $ D_{ii} $ by a factor of $\sqrt{(J^TJ)_{ii}}$. Solving for CERES value of 
$ c_{ii} $

$$c_{ii} = \frac{d_{ii}}{s_{ii}} = \frac{1}{\sqrt{\mbox{radius}}}\frac{\sqrt{(J^TJ)_{ii}}}{(1+\sqrt{(J^TJ)_{ii}})}(1+\sqrt{(J^TJ)_{ii}})$$

$$c_{ii} = \frac{\sqrt{(J^TJ)_{ii}}}{\sqrt{\mbox{radius}}}$$


So each element of $C^TC$ will just be $ctc_{ii} = \frac{(J^TJ)_{ii}}{\mbox{radius}}$. This is *not* dependent on the preconditioner $S$.

**This only holds true if S is recalculated every iteration, but in CERES it is NOT!**

To match CERES we cannot substitute in a value for $s_{ii}$ dependent on $(J^TJ)_{ii}$, since $J^TJ$ changes every non-linear iteration.

$$c_{ii} = \frac{d_{ii}}{s_{ii}} = \frac{\sqrt{k_{ii}/\mbox{radius}}}{s_{ii}}$$

$$c_{ii} = \frac{\sqrt{s_{ii}^2(J^TJ)_{ii}/\mbox{radius}}}{s_{ii}}$$

$$c_{ii} = \frac{s_{ii}\sqrt{(J^TJ)_{ii}/\mbox{radius}}}{s_{ii}}$$

$$c_{ii} = \frac{\sqrt{(J^TJ)_{ii}}}{\sqrt{\mbox{radius}}}$$

which is exactly the same as before, so we are good!





Double preconditioning
===============================================================================

Upon closer inspection, CERES actually has a second layer of preconditioning.
By default, it not only scales the Jacobian before any solving takes place,
it also then runs PCG on the resulting system, using the Jacobi preconditioner:

$$A\hat{x} = b$$

with $A = (JS)^T(JS) + D^TD$, $b = (JS)^TF$, $\hat{x} = S^{-1}\delta$.


The preconditioner is `diag(A)`, which we will call $K$ for now. Let $LL^T = K$

This is equivalent to solving the system:

$$L^{-1}AL^{-1T}\hat{x_c} = L^{-1}b$$

where $\hat{x_c} = L^T\hat{x}$

$$L^{-1}S(J^TJ + C^TC)S^TL^{-1T}\hat{x_c} = L^{-1}SJ^TF$$

and $\hat{x_c} = L^TS^{-1}\delta$.

This is itself equivalent to solving the system

$$(J^TJ + C^TC)\delta = J^TF$$

using PCG with 

$$M^{-1}=(L^{-1}S)^T(L^{-1}S)$$.

Each element of $M^{-1}$:

$$m^{-1}_{ii} = (\ell^{-1}_{ii})^2 (s_{ii})^2$$

Since $LL^T = K$, and both are diagonals, then $(\ell^{-1}_{ii})^2=k^{-1}_{ii}$,

so 

$$m^{-1}_{ii} = k^{-1}_{ii} (s_{ii})^2$$

Each element of $K$, 

$$k_{ii} = ((JS)^T(JS) + D^TD)_{ii}$$ 

$$k_{ii} = (S(J^TJ + C^TC)S^T)_{ii}$$ 

$$k_{ii} = s_{ii}^2(jtj_{ii} + ctc_{ii})$$

So:


$$m^{-1}_{ii} = \frac{1}{s_{ii}^2(jtj_{ii} + ctc_{ii})} s_{ii}^2$$

$$m^{-1}_{ii} = \frac{s_{ii}^2}{s_{ii}^2(jtj_{ii} + ctc_{ii})}$$

$$m^{-1}_{ii} = \frac{1}{(jtj_{ii} + ctc_{ii})}$$

Recall that $ctc_{ii} = \frac{(J^TJ)_{ii}}{\mbox{radius}}$, so the new preconditioner can be calculated as 

$$m^{-1}_{ii} = \frac{1}{((\mbox{radius})ctc_{ii} + ctc_{ii})}$$

$$m^{-1}_{ii} = \frac{1}{(1+\mbox{radius})ctc_{ii}}$$

*This is not quite right*. CtC is clamped using some min and max values. In Opt we compute $m^{-1}_{ii}$ in the same kernel as we clamp CtC, so we can use the proper formula easily:

 $$m^{-1}_{ii} = \frac{1}{((\mbox{radius})ctc^{unclamped}_{ii} + ctc_{ii})}$$



Now let us confirm that we are actually computing what we think we are computing in both Opt and CERES.

We should be able to confirm the relation between the actual values computed in Opt and CERES.

Recall Opt is solving


$$Ax = b$$

where $A=(J^TJ + C^TC)$, $x=\delta$, $b=J^TF$

$$(J^TJ + C^TC)\delta = J^TF$$

using PCG with 

$$M^{-1}=(L^{-1}S)^T(L^{-1}S)$$.

So going through all of the initialization parameters:

`b_Opt =` $J^TF$

`r_0_Opt = b_Opt - A_Opt x_0_Opt = b_Opt  =` $J^TF$

`preconditioner_Opt = `$M^{-1} = (L^{-1}S)^T(L^{-1}S)

`z_0_Opt = preconditioner_Opt * r_0_Opt =` $M^{-1}J^TF = (L^{-1}S)^T(L^{-1}S)J^TF$

`b_Opt =` $J^TF$

`r_0_Opt = b_Opt - A_Opt x_0_Opt = b_Opt  =` $J^TF$

`preconditioner_Opt = `$M^{-1} = (L^{-1}S)^T(L^{-1}S)$

`z_0_Opt = preconditioner_Opt * r_0_Opt =` $M^{-1}J^TF = (L^{-1}S)^T(L^{-1}S)J^TF$


On the other hand, CERES is solving

$$Ax = b$$

where $A=((JS)^T(JS) + D^TD)$, $x=\hat{x}=S^{-1}\delta$, $b=(JS)^TF$

$$((JS)^T(JS) + D^TD)\hat{x} = (JS)^TF$$

using PCG with 

$$M^{-1}=(L^{-1})^T(L^{-1})$$.

`b_CERES =` $(JS)^TF = S (J^TF) =$ `S * b_Opt`

`r_0_CERES = b_CERES - A_CERES x_0_CERES = b_CERES  = S * b_Opt`

`preconditioner_CERES = `$M^{-1} = (L^{-1})^T(L^{-1}) = S^{-1}S^{-1}(L^{-1}S)^T(L^{-1}S) =$ `(1/S*S) * preconditioner_Opt`

`z_0_CERES = preconditioner_CERES * r_0_CERES =` $M^{-1}(JS)^TF = S^{-1}S^{-1}(L^{-1}S)^T(L^{-1}S)(JS)^TF = S^{-1}(L^{-1}S)^T(L^{-1}S)J^TF =$ `(1/S) * preconditioner_Opt * b_Opt = (1/S) * z_0_Opt`



Adjusting the trust region radius, CERES vs. Opt
===============================================================================

CERES uses model_cost_change (lets call it $\delta_m$) to update the trust region radius at every LM step. Let cost = $c$ and new_model_cost = $c_m$. Let 

$$\delta_m = c - c_m$$

$$c = f^Tf$$

According to CERES:

$$c_m = (f + J \mbox{step})^2$$

but recall CERES has prebaked $S$ into $J$, and calculates the final $\delta$ as $\delta=S\mbox{step}$. So this is really:

$$c_m = (f + JS \mbox{step})^2 = (f + J\delta)^2$$

This is now implemented in Opt.


Known Outstanding Differences, CERES vs. Opt
===============================================================================

CERES controls when it stops the linear solver based on some protocol that cannot be directly transferred over to Opt, since we aren't solving the normal equations like CERES is.

initial_trust_region_radius is an important parameter that should be set the same for both CERES and Opt; it is hardcoded for Opt.

min_relative_decrease is also important to keep the same, and is hardcoded for Opt.

max_trust_region_radius and min_trust_region_radius are not yet implemented in Opt, but are straightforward

function_tolerance and gradient_tolerance are useful options for CERES that should be looked at for integration into Opt, as it allows us to early-exit

min_lm_diagonal and max_lm_diagonal are the last known discrepency between Opt and CERES, for now we just set them to extreme values for CERES and hope never to hit it.

Those two parameters operate on individual elements of $D$. Since we use $C'C$ where $D = CS$, 
to apply this we would need to clamp $c_{ii}$ to:

$$\frac{\mbox{min_lm_diagonal}}{s_{ii}} \le c_{ii} \le \frac{\mbox{max_lm_diagonal}}{s_{ii}}$$

which is clamping 

$$\frac{\mbox{min_lm_diagonal}^2}{s_{ii}^2} \le \mbox{ctc}_{ii} \le \frac{\mbox{max_lm_diagonal}^2}{s_{ii}^2}$$

 This isn't terribly difficult and so should be implemented for parity with CERES.


Opt has the user set the number of linear iterations for each PCG solve. CERES, on the other hand, terminates using a $q_\mbox{tolerance}$ value, which defaults to $0.1$. 

From comments in the CERES source code:

For PSD matrices $A$, let

$$Q(x) = x^TAx - 2b^Tx$$

be the cost of the quadratic function defined by $A$ and $b$. Then,
the solver terminates at iteration $i$ if:

$$i \frac{ Q(x_i) - Q(x_i-1)}{Q(x_i)} < q_\mbox{tolerance}$$

The CERES linear solver is solving the equations: $A = ((JS)^T(JS) + D^TD)$ and $b=(JS)^TF$.

CERES computes $Q(x_i)$ in the following line of code.

`double Q0 = -1.0 * xref.dot(bref + r);`

`bref` is $b=(JS)^TF$. `xref` is $\hat{x}=S^{-1}\delta$. `r` is $b-A\hat{x}$.

`double Q0 = -1.0 * xref.dot(bref + r);` is then $Q_0 = -\hat{x}\cdot (b + b-A\hat{x})$.

$$Q_0 = -\hat{x}\cdot (b + b-A\hat{x})$$

$$Q_0 = -\hat{x}\cdot (2b-A\hat{x})$$

$$Q_0 = -\hat{x}\cdot (2(JS)^TF-((JS)^T(JS) + D^TD)\hat{x})$$

$$Q_0 = -\hat{x}\cdot (2(JS)^TF-((JS)^T(JS) + (CS)^T(CS))\hat{x})$$

$$Q_0 = -(S^{-1}\delta) \cdot (2(JS)^TF-((JS)^T(JS) + (CS)^T(CS))S^{-1}\delta)$$

$$Q_0 = -(S^{-1}\delta)\cdot (2S^TJ^TF-(S^TJ^TJS + S^TC^TCS)S^{-1}\delta)$$

$$Q_0 = -(S^{-1}\delta)\cdot (S^T(2J^TF)-S^T((J^TJ + C^TC)SS^{-1}\delta))$$

$$Q_0 = -(S^{-1}\delta)\cdot S^T((2J^TF)-((J^TJ + C^TC)\delta))$$

$$Q_0 = -(S^{-1}\delta)\cdot S^T((2J^TF)-((J^TJ + C^TC)\delta))$$


Each element of the dot product is then:

$$s_i^{-1}\delta_i s_i ((2J^TF)-((J^TJ + C^TC)\delta))_i = \delta_i((2J^TF)-((J^TJ + C^TC)\delta))_i$$

This means

$$Q_0 = -\delta\cdot((2J^TF)-((J^TJ + C^TC)\delta))$$

In Opt, $J^TF$ is stored in `b`, `r` stores `b-Adelta`, which is equivalent to $J^TF-(J^TJ + C^TC)\delta$, so, we can compute 

`Q0 = delta.dot(r + b)


They explicitly don't do $r_\mbox{tolerance}$ checking since "As Nash and Sofer show, r_tolerance based termination is essentially useless in Truncated Newton methods."


The CERES solver also explicitly recomputes the residual directly ($Ax-b$) every `residual_reset_period` iterations (which defaults to 10), to prevent drift from round off error. Opt never does this.


Important Note
===============================================================================

Opt requires that all unknowns have some image-based residual (make a dummy one for now!); otherwise the diagonal matrix is not properly added!

http://www.itl.nist.gov/div898/strd/nls/nls_main.shtml

<!-- Markdeep: --><style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style><script src="markdeep.min.js"></script><script src="https://casual-effects.com/markdeep/latest/markdeep.min.js"></script><script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>
