                <meta charset="utf-8" emacsmode="-*- markdown -*-">
                            **CERES vs Opt**


One Iteration, CERES vs. Opt
===============================================================================


CERES directly modifies the Jacobian by multiplying it with a scaling matrix S.
S is a diagonal matrix where each entry 

$$s_{ii} = \frac{1}{1+\sqrt{(J^TJ)_{ii}}}$$

We can write this without resorting to entry-wise notation if we let

$$Q = \mbox{diag}(J^TJ)$$

and $U$ be the matrix such that

$$Q = U^TU$$

Then $S$ is just

$$S = (U + I)^{-1}$$

The following uses notation from:
https://en.wikipedia.org/wiki/Conjugate_gradient_method (The preconditioned conjugate gradient method)

Opt uses this notation internally (and uses a PCG solver), so this will be helpful to align our thoughts.

Using the preconditioner $M$, the preconditioned conjugate gradient solver is equivalent to solving:

$$E^{-1}A(E^{-1})^T\hat{x} = E^{-1}b$$

where $EE^T=M$ and $\hat{x} = E^Tx$.

I propose that CERES is solving exactly this system (and transforming $\hat{x}$ appropriately after solving), for:

$E^{-1} = S$, $A = J^TJ + \lambda I$, $\lambda = 1/\mbox{radius}$, $b = J^Tr$

CERES claims to be solving the system 
$$(J^TJ + D^TD)\hat{x} = J^Tr$$

but their J has already been multiplied by $S$, so the system really is 

$$((JS)^T(JS) + D^TD)\hat{x} = (JS)^Tr$$

Let $C$ be the matrix such that $D = CS$ ($S$ and $D$ are positive diagonal matrices, so such a $C$ is guaranteed to exist).

$$((JS)^T(JS) + (CS)^T(CS))\hat{x} = (JS)^Tr$$

since $(AB)^T = A^TB^T$, we have

$$(S^TJ^TJS + S^TC^TCS)\hat{x} = S^TJ^Tr$$

by distributivity:

$$S^T(J^TJ + C^TC)S\hat{x} = S^TJ^Tr$$

$S$ is again, diagonal, so $S^T = S$, thus:

$$S(J^TJ + C^TC)S^T\hat{x} = SJ^Tr$$

So CERES is solving the system $E^{-1}A(E^{-1})^T\hat{x} = E^{-1}b$

with $E^{-1} = S$, $A = J^TJ + C^TC$, and $b = J^Tr$.

This is close to what I proposed; they would be equivalent if $C^TC = I/\mbox{radius}$.

Let's see if this is indeed the case. Assume $C^TC = I/\mbox{radius}$.

We know each (diagonal) entry of $C^TC$ is $1/\mbox{radius}$, thus each diagonal entry of $C$ is $1/\sqrt{\mbox{radius}}$.

$CS = D$ and $s_{ii} = \frac{1}{1+\sqrt{(J^TJ)_{ii}}}$, then:


$$d_{ii} = c_{ii}s_{ii} = \frac{1}{\sqrt{\mbox{radius}}} \frac{1}{1+\sqrt{(J^TJ)_{ii}}}$$

$$d_{ii} = \frac{1}{\sqrt{\mbox{radius}}(1+\sqrt{(J^TJ)_{ii}})}$$

Let's take a look at CERES internals to see how it actually calculates $d_{ii}$.

It calculates:

$$K = diag((JS)^T(JS))$$

which is equivalently

$$K = diag(S^T(J^TJ)S)$$

Then

$$k_{ii} = s_{ii}^2(J^TJ)_{ii}$$

$$d_{ii} = \sqrt{k_{ii}/\mbox{radius}} = \sqrt{s_{ii}^2(J^TJ)_{ii}/\mbox{radius}}$$

$$d_{ii} = \sqrt{\frac{1}{1+\sqrt{(J^TJ)_{ii}}}^2(J^TJ)_{ii}/\mbox{radius}}$$

$$d_{ii} = \frac{1}{\sqrt{\mbox{radius}}}\sqrt{\frac{(J^TJ)_{ii}}{(1+\sqrt{(J^TJ)_{ii}})^2}}$$

$$d_{ii} = \frac{1}{\sqrt{\mbox{radius}}}\frac{\sqrt{(J^TJ)_{ii}}}{(1+\sqrt{(J^TJ)_{ii}})}$$

This is off from being the proposed value of $ D_{ii} $ by a factor of $\sqrt{(J^TJ)_{ii}}$. Solving for CERES value of 
$ c_{ii} $

$$c_{ii} = \frac{d_{ii}}{s_{ii}} = \frac{1}{\sqrt{\mbox{radius}}}\frac{\sqrt{(J^TJ)_{ii}}}{(1+\sqrt{(J^TJ)_{ii}})}(1+\sqrt{(J^TJ)_{ii}})$$

$$c_{ii} = \frac{\sqrt{(J^TJ)_{ii}}}{\sqrt{\mbox{radius}}}$$


So each element of $C^TC$ will just be $ctc_{ii} = \frac{(J^TJ)_{ii}}{\mbox{radius}}$. This is *not* dependent on the preconditioner S.


Adjusting the trust region radius, CERES vs. Opt
===============================================================================

CERES uses model_cost_change (lets call it $\delta_m$) to update the trust region radius at every LM step. Let cost = $c$ and new_model_cost = $c_m$. Let 

$$\delta_m = c - c_m$$

$$c = f^Tf$$

According to CERES:

$$c_m = (f + J \mbox{step})^2$$

but recall CERES has prebaked $S$ into $J$, and calculates the final $\delta$ as $\delta=S\mbox{step}$. So this is really:

$$c_m = (f + JS \mbox{step})^2 = (f + J\delta)^2$$

This is now implemented in Opt.


Known Outstanding Differences, CERES vs. Opt
===============================================================================

CERES controls when it stops the linear solver based on some protocol that cannot be directly transferred over to Opt, since we aren't solving the normal equations like CERES is.

initial_trust_region_radius is an important parameter that should be set the same for both CERES and Opt; it is hardcoded for Opt.

min_relative_decrease is also important to keep the same, and is hardcoded for Opt.

max_trust_region_radius and min_trust_region_radius are not yet implemented in Opt, but are straightforward

function_tolerance and gradient_tolerance are useful options for CERES that should be looked at for integration into Opt, as it allows us to early-exit

min_lm_diagonal and max_lm_diagonal are the last known discrepency between Opt and CERES, for now we just set them to extreme values for CERES and hope never to hit it.

Those two parameters operate on individual elements of $D$. Since we use $C'C$ where $D = CS$, 
to apply this we would need to clamp $c_{ii}$ to:

$$\frac{\mbox{min_lm_diagonal}}{s_{ii}} \le c_{ii} \le \frac{\mbox{max_lm_diagonal}}{s_{ii}}$$

which is clamping 

$$\frac{\mbox{min_lm_diagonal}^2}{s_{ii}^2} \le \mbox{ctc}_{ii} \le \frac{\mbox{max_lm_diagonal}^2}{s_{ii}^2}$$

 This isn't terribly difficult and so should be implemented for parity with CERES.


Opt has the user set the number of linear iterations for each PCG solve. CERES, on the other hand, terminates using a $q_\mbox{tolerance}$ value, which defaults to $0.1$. 

From comments in the CERES source code:

For PSD matrices $A$, let

$$Q(x) = x^TAx - 2b^Tx$$

be the cost of the quadratic function defined by $A$ and $b$. Then,
the solver terminates at iteration $i$ if:

$$i \frac{ Q(x_i) - Q(x_i-1)}{Q(x_i)} < q_\mbox{tolerance}$$

The CERES linear solver is solving the equations: $A = ((JS)^T(JS) + D^TD)$ and $B=(JS)^Tr$$.

They explicitly don't do $r_\mbox{tolerance}$ checking since "As Nash and Sofer show, r_tolerance based termination is essentially useless in Truncated Newton methods."

The CERES solver also explicitly recomputes the residual directly ($Ax-b$) every residual_reset_period iterations (which defaults to 10), to prevent drift from round off error. Opt never does this.


Important Note
===============================================================================

Opt requires that all unknowns have some image-based residual (make a dummy one for now!); otherwise the diagonal matrix is not properly added!

<!-- Markdeep: --><style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style><script src="markdeep.min.js"></script><script src="https://casual-effects.com/markdeep/latest/markdeep.min.js"></script><script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>
