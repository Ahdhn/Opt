%%% template.tex
%%%
%%% This LaTeX source document can be used as the basis for your technical
%%% paper or abstract.

%%% The parameter to the ``documentclass'' command is very important.
%%% - use ``review'' for content submitted for review.
%%% - use ``preprint'' for accepted content you are making available.
%%% - use ``tog'' for technical papers accepted to the TOG journal and
%%%   for presentation at the SIGGRAPH or SIGGRAPH Asia conference.
%%% - use ``conference'' for final content accepted to a sponsored event
%%%   (hint: If you don't know, you should use ``conference.'')

%\documentclass[tog]{acmsiggraph}
\documentclass[review]{acmsiggraph}

%%% Make the ``BibTeX'' word pretty...

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

%%% Used by the ``review'' variation; the online ID will be printed on 
%%% every page of the content.

\TOGonlineid{0011}

%%% Used by the ``preprint'' variation.

\TOGvolume{0}
\TOGnumber{0}

\usepackage{comment}
\usepackage{bm}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{microtype}
\usepackage{color}
\usepackage{listings}
\usepackage{algpseudocode}

\input{macros}

\title{OptiSL: Real-time Non-Linear Least Squares}

\author{Stephen N. Spencer\thanks{e-mail:spencer@cs.washington.edu}\\Chair, ACM SIGGRAPH Publications Committee}
\pdfauthor{Stephen N. Spencer}

\keywords{radiosity, global illumination, constant time}



\begin{document}

%%% This is the ``teaser'' command, which puts an figure, centered, below 
%%% the title and author information, and above the body of the content.

 \teaser{
   %\includegraphics[height=1.5in]{images/sampleteaser}
   \caption{Caption}
 }

\maketitle

\begin{abstract}


\end{abstract}

\begin{CRcatlist}
  \CRcat{I.3.3}{Computer Graphics}{Three-Dimensional Graphics and Realism}{Display Algorithms}
  \CRcat{I.3.7}{Computer Graphics}{Three-Dimensional Graphics and Realism}{Radiosity};
\end{CRcatlist}

\keywordlist

%% Required for all content. 

\copyrightspace

\section{Gauss-Newton Optimization}
$n$ variables\\
$k$ residuals\\
$\mathbf{x}$ unknown\\
Traditional Gauss-Newton definition
$$E (\mathbf{x}) = || \mathbf{F}(\mathbf{x}) ||_2^2,\qquad \mathbf{F}(\mathbf{x})  =[f_1(\mathbf{x}), \hdots, f_k(\mathbf{x})]^T$$



The optimal parameters $\mathbf{x}^{*}$ are obtained by solving the minimization problem
$$ \mathbf{x}^{*} = \displaystyle \argmin_\mathbf{x} \Vert \mathbf{F} (\mathbf{x}) \Vert_2^2.$$
%
We linearize the vector field $\mathbf{F}(\mathbf{x})$ using the first-order Taylor expansion,
and obtain a linearized solution $\mathbf{x}_{k+1}$ for a given $\mathbf{x}_k$:
%and obtain an iterative update $\mathbf{x}_k \rightarrow \mathbf{x}_{k+1}$:
$$ \mathbf{F}(\mathbf{x}_{k+1}) \approx \mathbf{F}(\mathbf{x}_{k})+\mathbf{J}(\mathbf{x}_{k})\delta_k,\qquad \delta_k = \mathbf{x}_{k+1}-\mathbf{x}_{k},$$
where $\mathbf{J}(\mathbf{x}_k)$ is the Jacobian matrix of $\mathbf{F}$ evaluated at $\mathbf{x}_k$.
This approximation transforms the original non-linear optimization problem into a linear minimization problem:
$$
\mathbf{\delta}_{k}^{*}=\argmin_{\mathbf{\delta}_{k}}{||\mathbf{F}(\mathbf{x}_{k})+\mathbf{J}(\mathbf{x}_{k})\mathbf{\delta}_{k}||^2}.
$$
%where $\mathbf{\delta}_k^{*}$ is the optimal solution.
This is a highly over-constrained linear system for which we obtain the optimal least-squares solution $\mathbf{\delta}_k^{*}$ by solving the corresponding normal equations:
$$
2 \mathbf{J}(\mathbf{x}_{k})^T\mathbf{J}(\mathbf{x}_{k})\mathbf{\delta}_{k}^{*} = - 2\mathbf{J}(\mathbf{x}_{k})^T\mathbf{F}(\mathbf{x}_{k}).
$$
To solve the original non-linear minimization problem, we thus need to solve a sequence of linearized problems;
i.e., we initialize $\mathbf{x}_0$, and successively compute the update $\mathbf{\delta}_k^{*}$ from $\mathbf{x}_k$ to $\mathbf{x}_{k+1}$.

\section{Applications}



\subsection{Laplacian Smoothing} 
$$E_{\mathit{fit}}(i,j) = [I(i,j) - \hat{I}(i,j) ]^2$$

$$E_{\mathit{reg}}(i,j) = \sum_{(l,m) \in \mathcal{N}(i,j)} [I(i,j) - I(l,m) ]^2,$$
$$\textnormal{where}\ \mathcal{N}(i,j) = \{(i+1,j), (i-1,j), (i,j+1), (i,j-1)\}$$

$$E_{\Delta} = \sum_{(i,j) \in \mathcal{I}} w_{\mathit{fit}} E_{\mathit{fit}}(i,j) + w_{\mathit{reg}} E_{\mathit{reg}}(i,j)$$

\subsection{Bi-Laplacian Smoothing}

$$E_{\mathit{fit}}(i,j) = [I(i,j) - \hat{I}(i,j) ]^2$$

$$E_{\mathit{reg}}(i,j) = [4 I(i,j) - I(i+1,j) - I(i-1,j) - I(i,j+1) - I(i,j-1)]^2$$

$$E_{\Delta^2} = \sum_{(i,j) \in \mathcal{I}} w_{\mathit{fit}} E_{\mathit{fit}}(i,j) + w_{\mathit{reg}} E_{\mathit{reg}}(i,j)$$

\subsection{Poisson Image Editing}

\cite{perez2003poisson}

\subsection{Shape from Shading (Image)}

\cite{wu2014sfs}

\subsection{Shape from Shading (Volume)}

\subsection{Non-rigid Surface Tracking (Graph)}

\cite{zollhoefer2014deformable}



\section{Results}

\subsection{Comparison with CUSparse PCG Solver Reference Implementation}

CUSparse can not:
\begin{itemize}
\item Exploit shared memory (locality)
\item Has to read weights from matrix (can not compute them on the fly), this is bad for the GPU
\item Has to navigate to neighbour variables by traversing (CRS), even so the indices could be computed. Therefore more expensive.
\item More kernel calls (can not interleave stages of the PCG), we minimize sync points
\item Bad if system matrix changes alot (GN Solver!), since matrix has to be build each time, before solving
\item It is even worse if the non zero structure (layout of the CRS changes), this is normally the case for articulated/non-rigid ICP variants.
\item No batched/block reads, for example for mesh vertices, this results in 3 float reads instead of one float3/4 read
\end{itemize}


%\section*{Acknowledgements}

\bibliographystyle{acmsiggraph}
\nocite{*}
\bibliography{dsl}
\end{document}
